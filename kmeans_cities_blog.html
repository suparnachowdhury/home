<!DOCTYPE HTML>

<html>
	<head>
        <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RFC48RMEYG"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-RFC48RMEYG');
</script>
		<title>Suparna Chowdhury</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

				<!-- Header -->
		<section id="header">
			<!-- Ribbon inserted at the top -->
			<div class="header-ribbon">
				<div class="ribbon-wrapper">
					<a href="https://github.com/suparnachowdhury" class="logo-link">
						<img alt="" src="images/profile_picture.png" class="ribbon-img">
						<div class="ribbon-text">
							<div class="name">Suparna Chowdhury</div>
							
						</div>
					</a>
		
					<nav class="ribbon-nav">
						<a href="index.html" class="ribbon-link">Home</a>
						<a href="resume.html" class="ribbon-link">About Me</a>
						<a href="portfolio.html" class="ribbon-link">Portfolio</a>
						<a href="resources.html" class="ribbon-link">Resources</a>
						<a href="https://suparnachowdhury.medium.com/" target="_blank" class="ribbon-link">Blog</a>
						<a href="https://linkedin.com/in/suparna-chowdhury" target="_blank" class="ribbon-cta">
							<img src="images/profile_pic.jpg" alt="" class="ribbon-img">
							<div>Let's Connect</div>
						</a>
						
					</nav>					
				</div>
			</div>	
		</section>

			<!-- Main -->
				<section id="main" class="container">
					
					<div class="box">
						<h1>Understanding K-Means Clustering: A Simple Guide</h1>

    <p>A beginner-friendly introduction to one of the most popular clustering algorithms in machine learning.</p>

    <p>In machine learning, <strong>K-Means clustering</strong> is one of the most popular algorithms for uncovering hidden patterns in data. It is an <strong>unsupervised learning technique</strong>—meaning it does not rely on labeled data—and is often used for tasks like customer segmentation, image compression, and pattern recognition.</p>

    <hr>

    <h2>What Is K-Means Clustering?</h2>

    <p>K-Means clustering is an algorithm used to split a dataset into $\text{K}$ distinct groups, or <strong>clusters</strong>. The goal is to group together data points that are similar to each other while keeping different clusters as distinct as possible. The term “means” refers to the average position of the points in each cluster—known as the <strong>centroid</strong>, which represents the cluster’s center.</p>

    <hr>

    <h2>How It Works</h2>

    <ol>
        <li><strong>Choose $\text{K}$:</strong> Decide how many clusters you want to form.</li>
        <li><strong>Initialize centroids:</strong> Randomly select $\text{K}$ data points to act as the starting centers.</li>
        <li><strong>Assign points:</strong> Each data point is assigned to the nearest centroid based on distance (usually Euclidean). </li>
        <li><strong>Update centroids:</strong> For each cluster, calculate a new centroid by taking the mean of all points assigned to it.</li>
        <li>Continue reassigning points and updating centroids until the clusters stabilize (i.e., no major changes occur).</li>
    </ol>

    <hr>

    <h2>Implementing K-Means in Python</h2>

    <p>We will use Python’s <code>scikit-learn</code> library to implement K-Means. First, let’s import the necessary libraries:</p>

    <div class="code-block">
        <pre><code>from sklearn.cluster import KMeans

# Initialize KMeans with 4 clusters
kmeans = KMeans(n_clusters=2, n_init = 'auto', random_state=42)</code></pre>
    </div>

    <p>Let’s break down the parameters:</p>
    <ul>
        <li><strong><code>n_clusters=2</code></strong>
            <p>This sets the number of clusters ($\text{K}$) the algorithm will try to find in the data. K-Means will partition the data into 2 groups. The default value in scikit-learn is <code>n_clusters=8</code>.</p>
        </li>
        <li><strong><code>n_init='auto'</code></strong>
            <p>This controls how many times the K-Means algorithm will run with different centroid seeds. K-Means can converge to local minima, so running it multiple times improves the chances of finding a better solution. <code>'auto'</code>: scikit-learn decides how many times to run. If the <code>init</code> parameter is <code>'random'</code>, it runs 10 times; if <code>init</code> is <code>'k-means++'</code> (the default), it runs once.</p>
        </li>
        <li><strong><code>random_state=42</code></strong>
            <p>This sets the seed for random number generation, ensuring reproducibility. Without it, you might get slightly different clusters each time the algorithm runs.</p>
        </li>
    </ul>

    <p>I am using City Lifestyle dataset from Kaggle for this demo.</p>

    <div class="code-block">
        <pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


df = pd.read_csv('city_lifestyle_dataset.csv')
df.head()

df.shape


# Check for missing values
df.isnull().sum()

# check datatypes
df.dtypes

# feature selection
data = df.drop(columns=['city_name', 'country'])
data.head()

data.describe()</code></pre>
    </div>

    <hr>

    <h2>Why Feature Scaling is Important in K-Means</h2>

    <p>K-Means clustering uses **Euclidean distance** to assign points to clusters. This means that features with larger numerical ranges will **dominate** the distance calculation, potentially skewing the clustering results.</p>

    <p>From the descriptive statistics (not fully shown, but implied):</p>
    <ul>
        <li><code>population_density</code> ranges from 100 to 14,427 (Standard Deviation $\sim 2,983$).</li>
        <li><code>happiness_score</code> ranges from 2.5 to 8.5 (Standard Deviation $\sim 1.69$).</li>
    </ul>
    <p>Without scaling, the large values of <code>population_density</code> would overwhelm other features in the distance calculation. K-Means would essentially cluster the data based on population density alone.</p>

    <p>By applying **feature scaling (Standardization)**, all features are brought to a comparable scale, ensuring:</p>
    <ul>
        <li><strong>Equal contribution</strong> of all features to the distance metric.</li>
        <li><strong>Better cluster quality</strong>, allowing K-Means to detect multi-feature patterns.</li>
        <li><strong>Stability and convergence</strong>.</li>
    </ul>

    <div class="code-block">
        <pre><code>from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(data)</code></pre>
    </div>

    <hr>

    <h2>K-Means Algorithm Execution $(\text{K}=3)$</h2>

    <p>Deciding the optimal number of clusters ($\text{K}$) is addressed later, but we will initially choose 3 clusters for demonstration.</p>

    <div class="code-block">
        <pre><code>from sklearn.cluster import KMeans

# Create a K-Means model with 3 clusters
kmeans3 = KMeans(n_clusters=3, n_init='auto', random_state=42)

# Fit the model to the scaled data
kmeans3.fit(X_scaled)

# Get the cluster labels for each data point
labels = kmeans3.labels_

# Get the coordinates of the cluster centers
cluster_centers = kmeans3.cluster_centers_ </code></pre>
    </div>

    <h3>Visualization (2 Features vs. PCA)</h3>

    <p>Plotting the first two scaled features (Population Density vs. Avg Income) and then using Principal Component Analysis (PCA) to reduce the data to 2 dimensions for a better, more representative view of the cluster structure.</p>

    <div class="code-block">
        <pre><code>plt.figure(figsize=(8,6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis', s=50)
plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', s=200, alpha=0.75, marker='X')  # Centroids
plt.xlabel('Population Density (Scaled)')
plt.ylabel('Avg Income (Scaled)')
plt.title('K-Means Clustering (Two Features)')
plt.show()</code></pre>
        
    </div>

    <div class="code-block">
        <pre><code>from sklearn.decomposition import PCA

#PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', s=50)
plt.scatter(pca.transform(cluster_centers)[:, 0], pca.transform(cluster_centers)[:, 1], c='red', s=200, alpha=0.75, marker='X')
plt.title('K-Means Clustering (PCA Reduced)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()</code></pre>
        
    </div>

    <hr>

    <h2>Interpreting K-Means Clustering ($\text{K}=3$)</h2>

    <h3>Step 1: Understanding the Centroids</h3>

    <p>Since we used <code>StandardScaler</code> (mean 0, standard deviation 1), a **positive value** in the centroid means that feature is **above** the dataset’s average, and a **negative value** means it is **below** the dataset’s average. The heatmap visualization is essential here:</p>

    <div class="code-block">
        <pre><code>import seaborn as sns
sns.heatmap(cluster_centers3, annot= True, cmap = 'RdBu')</code></pre>
        
    </div>

    <h3>Step 2 & 3: Interpreting and Defining Each Cluster</h3>

    <table>
        <thead>
            <tr>
                <th>Cluster ID</th>
                <th>Key Centroid Characteristics (Scaled)</th>
                <th>Interpretation & Profile</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Cluster 0</strong></td>
                <td>High population density (1.467) and high air pollution (1.039). Lower than average income, rent, happiness, and green space.</td>
                <td><strong>High-Density Urban Stress Zones:</strong> Likely represents dense urban areas with poorer environmental quality and slightly lower quality of life metrics.</td>
            </tr>
            <tr>
                <td><strong>Cluster 1</strong></td>
                <td>Low income, low internet penetration, low rent (all &lt;-1). Slightly above average air quality (0.291). Low public transport score and low happiness.</td>
                <td><strong>Low-Income Underserved Regions:</strong> Possibly less developed or rural areas with lower amenities and lower overall happiness and economic vitality.</td>
            </tr>
            <tr>
                <td><strong>Cluster 2</strong></td>
                <td>Above-average income, internet penetration, rent, happiness, and public transport score. Lower population density and better air quality (-0.692). Moderate green space.</td>
                <td><strong>Affluent High-Quality Living Areas:</strong> Likely suburban or developed areas with good quality of life, strong infrastructure, and higher economic status.</td>
            </tr>
        </tbody>
    </table>

    <hr>

    <h2>Deciding the Best Number of Clusters ($\text{K}$)</h2>

    <h3>Inertia (Within-Cluster Sum of Squares)</h3>

    <p><strong>Inertia</strong> is a metric that measures the compactness of the clusters—how close the data points within each cluster are to their assigned cluster center (centroid). Mathematically, it is the sum of squared distances between each data point and the centroid of its cluster. The goal of K-Means is to **minimize inertia**.</p>

    <blockquote>
        <p>With 2 clusters, inertia is 1411.05. Increasing to 3 clusters reduces it to 1036, showing that 3 clusters are more compact. Since inertia always decreases as $\text{K}$ increases, it cannot be used alone to determine the optimal $\text{K}$ without further analysis.</p>
    </blockquote>

    <h3>The Elbow Method</h3>

    <p>The Elbow Method plots the inertia for a range of $\text{K}$ values. The **"elbow"** is the point where the inertia curve flattens, indicating that adding more clusters provides only small, diminishing returns in terms of compactness.</p>

    <div class="code-block">
        <pre><code>inertia_values = []
k_values = range(2,12) 

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia_values.append(kmeans.inertia_)
# Code to plot the elbow curve
# plt.plot(k_values, inertia_values, marker='o')
# plt.xlabel('Number of Clusters (K)')
# plt.ylabel('Inertia')
# plt.title('Elbow Method for Optimal K')
# plt.show()</code></pre>
        
    </div>

    <p>In this example, two potential elbows were identified at $\text{K}=3$ and $\text{K}=5$.</p>

    <h3>Comparison: $\text{K}=5$ Interpretation</h3>

    <div class="code-block">
        <pre><code>kmeans5 = KMeans(n_clusters=5, n_init='auto', random_state=42)
kmeans5.fit(X_scaled)

labels = kmeans5.labels_
cluster_centers5 = pd.DataFrame(kmeans5.cluster_centers_, columns=data.columns)
sns.heatmap(cluster_centers5, annot= True, cmap = 'RdBu')</code></pre>
        
    </div>

    <p>Compared to $\text{K}=3$, $\text{K}=5$ reveals more detailed subgroups (finer segmentation) but adds complexity. If simplicity and interpretability are priorities, $\text{K}=3$ might still be sufficient.</p>

    <hr>

    <h2>Fine-Tune our K-Means Model (Feature Engineering)</h2>

    <p>To improve model performance and interpretability, we can create new features that capture complex relationships, such as economic efficiency or livability.</p>

    <div class="code-block">
        <pre><code># Create engineered features
data['income_per_density'] = data['avg_income'] / data['population_density']
data['rent_to_income_ratio'] = data['avg_rent'] / data['avg_income']
data['urbanization_score'] = (
                 (data['population_density'] * 0.3) + 
                 (data['public_transport_score'] * 0.3) + 
                 (data['internet_penetration'] * 0.2) - 
                 (data['green_space_ratio'] * 0.2))
data['quality_of_life'] = (data['happiness_score'] + data['green_space_ratio'] + data['public_transport_score']) / 3
data['environment_comfort'] = ( (100 - data['air_quality_index']) +  data['green_space_ratio']) / 2
data['mobility_index'] = data['public_transport_score'] / data['population_density']
# Select features for scaling
features_to_scale = ['income_per_density', 'rent_to_income_ratio',
      'urbanization_score', 'quality_of_life','environment_comfort','mobility_index']

# Handle division by zero or infinite values if any
data.replace([np.inf, -np.inf], np.nan, inplace=True)
data.fillna(data.mean(), inplace=True)

# Scale features
scaler = StandardScaler()
X_scaled_v2 = scaler.fit_transform(data[features_to_scale])</code></pre>
    </div>

    <p>With the new features, the Elbow Method now suggests an optimal $\text{K}=4$.</p>

    <div class="code-block">
        <pre><code>kmeans4 = KMeans(n_clusters=4, n_init='auto', random_state=42)
kmeans4.fit(X_scaled_v2)


labels = kmeans4.labels_
cluster_centers4 = pd.DataFrame(kmeans4.cluster_centers_, columns=data[features_to_scale].columns)
sns.heatmap(cluster_centers4, annot= True, cmap = 'RdBu')</code></pre>
        
    </div>

    <h3>Final Cluster Profiling and Interpretations ($\text{K}=4$)</h3>

    <table>
        <thead>
            <tr>
                <th>Cluster ID</th>
                <th>Cluster Name</th>
                <th>Key Characteristics</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Cluster 0</strong></td>
                <td>Stressed, High-Cost Areas</td>
                <td>Low on environmental quality and economic density, but burdened by the **highest housing costs relative to income** (high <code>rent_to_income_ratio</code>), suggesting struggling smaller cities or expensive suburbs.</td>
            </tr>
            <tr>
                <td><strong>Cluster 1</strong></td>
                <td>Elite, High-Efficiency Areas</td>
                <td>Defined by unparalleled wealth relative to density (**highest** <code>income_per_density</code>) and exceptional transit efficiency (high <code>mobility_index</code>), creating the best overall environment.</td>
            </tr>
            <tr>
                <td><strong>Cluster 2</strong></td>
                <td>Dense, High-Congestion Metropolises</td>
                <td>The hyper-dense urban core (highest <code>urbanization_score</code>), which suffers from extremely **poor environmental comfort** (low score) and overwhelmed transit infrastructure (low <code>mobility_index</code>).</td>
            </tr>
            <tr>
                <td><strong>Cluster 3</strong></td>
                <td>Balanced, Highly Livable Cities</td>
                <td>Excels across **quality-of-life** and **environmental** metrics (highest scores), representing a desirable, well-managed city with moderate density and cost.</td>
            </tr>
        </tbody>
    </table>

						
					</div>
				</section>

		

		</div>
			<!-- Footer -->
				<footer id="footer">
					<ul class="icons">
						<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
						<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
						<li><a href="#" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li><a href="#" class="icon brands fa-dribbble"><span class="label">Dribbble</span></a></li>
						<li><a href="#" class="icon brands fa-google-plus"><span class="label">Google+</span></a></li>
					</ul>
					<ul class="copyright">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>